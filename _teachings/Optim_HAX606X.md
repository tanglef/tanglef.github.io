---
title: HAX606X - Convex Optimization (2021-?)
falling:
  - image_path: /_teachings/data/HAX606X_optim/falling.svg
    alt: "surface"
---

This is an undergraduate course (in French!) introducing standard techniques from convex optimization. Numerical elements are provided in Python. Codes and questions are written with [Joseph Salmon](http://josephsalmon.eu).

$$f(x, y) = \frac{xy}{1+e^{x^2 - y^2}}$$
{% include feature_row id="falling" type="center" %}

# 2022-2023


<h2> TP0: Installations </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp0_quarto.html">[installation]</a> </li>
</ul>



<h2> TP1: Introduction to Python </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp1_quarto.html">[prise en main]</a> </li>
</ul>

<h2> TP2: Algorithmes d’optimisation 1D </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp2_quarto.html">[optimisation 1D]</a> </li>
</ul>

<h2> TP3: Méthode de descente de gradient </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp3_quarto.html">[sujet descente]</a> </li>
  <li> Fichiers widgets: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/dico_math_functions.py">[fonctions]</a> <a href="{{ site.url }}/_teachings/data/HAX606X_optim/widget_level_set.py">[widget_level_set]</a>   <a href="{{ site.url }}/_teachings/data/HAX606X_optim/widget_convergence.py">[widget_convergence]</a></li>
</ul>


# 2021-2022

<details>
  <summary>Click to expand</summary>

  <h2> TP1: Introduction to Python </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp1_sujet.pdf"> [pdf]</a></li>
  <li> code: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp1_sujet.py">[py]</a> </li>

  <h2> TP2: First 1D algorithms: bissection and golden search methods </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp2_sujet.pdf">[pdf]</a></li>

  <h2> TP3: Gradient descent and coordinate descent </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp3_sujet.pdf">[pdf]</a></li>
  <li> widgets: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/dico_math_functions.py">[fonctions]</a> <a href="{{ site.url }}/_teachings/data/HAX606X_optim/widget_level_set.py">[widget_level_set]</a>   <a href="{{ site.url }}/_teachings/data/HAX606X_optim/widget_convergence.py">[widget_convergence]</a></li>

  It is necessary to have an up-to-date version of matplotlib to run the widgets. Numba and Ipython are also used.
  This is the corner stone of the course !!

  <h2> TP4: Projected gradient descent and application </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/tp4_sujet.pdf">[pdf]</a></li>
  <li> widgets: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/dico_math_functions.py">[fonctions]</a> <a href="{{ site.url }}/_teachings/data/HAX606X_optim/widget_level_set.py">[widget_level_set]</a>   <a href="{{ site.url }}/_teachings/data/HAX606X_optim/widget_convergence.py"> [widget_convergence]</a> (same as TP3, but still relevant!) </li>
  <li> dataset: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/datasets/Iowa_Liquor_tp.csv">[iowa_alcohol]</a></li>
  <li> script with dataset: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/script_season.py">[alcohol_script]</a></li>

  The dataset available here is an already preprocessed and subdataset of the original IowaLiquor dataset (link in the alcohol script file).


</details>
