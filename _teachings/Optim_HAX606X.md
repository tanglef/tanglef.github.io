---
title: HAX606X - Convex Optimization (2021-2024)
falling:
  - image_path: /_teachings/data/HAX606X_optim/falling.svg
    alt: "surface"
---

This is an undergraduate course (in French!) introducing standard techniques from convex optimization. Numerical elements are provided in Python. Codes and questions are written with [Joseph Salmon](http://josephsalmon.eu) and [Amélie Vernay](https://github.com/AmelieVernay).

$$f(x, y) = \frac{xy}{1+e^{x^2 - y^2}}$$
{% include feature_row id="falling" type="center" %}

# 2023-2024

<h2> TP0: Installations </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp0.html">[installation]</a> </li>
</ul>

<h2> TP1: Introduction to Python </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp1.html">[prise en main]</a> </li>
</ul>

<h2> TP2: Algorithmes d’optimisation 1D </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp2.html">[optimisation 1D]</a> </li>
</ul>

<h2> TP3: Descentes de gradient et variantes </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp3.html">[Descentes de gradient]</a> </li>
<li> un corrigé proposé: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp3_corr.html">[Descentes de gradient (correction possible)]</a> </li>
</ul>

<h2> TP4: Descente de gradient projeté </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp4.html">[Descente de gradient projeté]</a> </li>
</ul>

<h2> TP5: Différentiation automatique avec Pytorch </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2024/tp5.html">[TP pytorch pour l'optimisation]</a> </li>
</ul>

<h2> Annales </h2>
<ul>
<li> sujet CT: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/ct2024.html">[CT2024]</a> </li>
</ul>

# 2022-2023

<details>
<summary> Click to expand </summary>
<h2> TP0: Installations </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp0_quarto.html">[installation]</a> </li>
</ul>

<h2> TP1: Introduction to Python </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp1_quarto.html">[prise en main]</a> </li>
</ul>

<h2> TP2: Algorithmes d’optimisation 1D </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp2_quarto.html">[optimisation 1D]</a> </li>
</ul>

<h2> TP3: Méthode de descente de gradient </h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp3_quarto.html">[sujet descente]</a> </li>
  <li> Fichiers widgets: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/dico_math_functions.py">[fonctions]</a> <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/widget_level_set.py">[widget_level_set]</a>   <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/widget_convergence.py">[widget_convergence]</a></li>
</ul>

<h2> TP4: Descente de gradient projeté</h2>
<ul>
<li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp4_quarto.html">[sujet descente de gradient projeté]</a> </li>
</ul>
</details>

<h2> Annales </h2>
<ul>
<li> sujet CC: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/CC2023.html">[CC2023]</a> </li>
<li> sujet CT: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/CT2023.html">[CT2023]</a> </li>
</ul>

# 2021-2022

<details>
  <summary>Click to expand</summary>

  <h2> TP1: Introduction to Python </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp1_sujet.pdf"> [pdf]</a></li>
  <li> code: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp1_sujet.py">[py]</a> </li>

  <h2> TP2: First 1D algorithms: bissection and golden search methods </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp2_sujet.pdf">[pdf]</a></li>

  <h2> TP3: Gradient descent and coordinate descent </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp3_sujet.pdf">[pdf]</a></li>
  <li> widgets: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/dico_math_functions.py">[fonctions]</a> <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/widget_level_set.py">[widget_level_set]</a>   <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/widget_convergence.py">[widget_convergence]</a></li>

  It is necessary to have an up-to-date version of matplotlib to run the widgets. Numba and Ipython are also used.
  This is the corner stone of the course !!

  <h2> TP4: Projected gradient descent and application </h2>

  <li> sujet: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/tp4_sujet.pdf">[pdf]</a></li>
  <li> widgets: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/dico_math_functions.py">[fonctions]</a> <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/widget_level_set.py">[widget_level_set]</a>   <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/widget_convergence.py"> [widget_convergence]</a> (same as TP3, but still relevant!) </li>
  <li> dataset: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/datasets/Iowa_Liquor_tp.csv">[iowa_alcohol]</a></li>
  <li> script with dataset: <a href="{{ site.url }}/_teachings/data/HAX606X_optim/2023/script_season.py">[alcohol_script]</a></li>

  The dataset available here is an already preprocessed and subdataset of the original IowaLiquor dataset (link in the alcohol script file).


</details>
